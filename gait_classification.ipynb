{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23174e5",
   "metadata": {},
   "source": [
    "# Gait Classification \n",
    "\n",
    "In this notebook we are going to achieve the following:\n",
    "\n",
    "1. **Create the Training Dataset**: We will preprocess and combine data from different sources to create a comprehensive training dataset.\n",
    "2. **Feature Selection and Dimensionality Reduction**: We will identify the most relevant features for classification using dimensionality reduction techniques.\n",
    "3. **Model Evaluation**: We will test and compare the performance of multiple machine learning and deep learning algorithms for gait classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d608453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pointbiserialr\n",
    "from data_preprocessing import classification_merge_all_types\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "\n",
    "# Constants\n",
    "healthy_dir = 'Data/Healthy'\n",
    "stroke_dir = 'Data/Stroke'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data file combining the results of feature extraction for both healthy and stroke patients\n",
    "classification_merge_all_types(healthy_dir, stroke_dir)\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "\n",
    "# Drop the column subject_id as we don't need it\n",
    "data = data.drop(columns=['subject_id'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53681d04",
   "metadata": {},
   "source": [
    "# Feature Selection and Dimensionality Reduction\n",
    "\n",
    "## 1. Point-biserial Correlation\n",
    "The point biserial correlation coefficient is a measure of the correlation between a binary variable (such as a yes/no or pass/fail variable) and a continuous variable. It is similar to the Pearson correlation coefficient, but is used specifically for this type of data. The point biserial correlation coefficient ranges from -1 to 1, with positive values indicating a positive correlation and negative values indicating a negative correlation. Values close to 0 indicate little or no correlation. The p-value represents the probability that the correlation between the two variables is due to chance. Typically, a p-value of less than 0.05 is considered to be statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data['label']\n",
    "features = data.drop(columns=['label'])\n",
    "\n",
    "correlations, p_values = [], []\n",
    "\n",
    "# Calculate the Point Biserial Correlation Coefficient for each feature\n",
    "for feature in features:\n",
    "    correlation, p_value = pointbiserialr(label, features[feature])\n",
    "    correlations.append(correlation)\n",
    "    p_values.append(p_value)\n",
    "\n",
    "# Sort the features by their correlation with the label and create dataframes for better visualization\n",
    "sorted_indices = np.argsort(correlations)[::-1]\n",
    "sorted_features = features.columns[sorted_indices]\n",
    "sorted_correlations = np.array(correlations)[sorted_indices]\n",
    "sorted_p_values = np.array(p_values)[sorted_indices]\n",
    "\n",
    "corr_df = pd.DataFrame({'Feature': sorted_features, 'Correlation': sorted_correlations, 'p-value': sorted_p_values})\n",
    "corr_df.dropna()\n",
    "corr_df = corr_df[corr_df['p-value'] < 0.05]\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e5180",
   "metadata": {},
   "source": [
    "Based on the results here we know that the feature **right-z-axis-(deg/s)-min** has a strong positive correlation of 0.77 and a p-value of 0.001 ensuring that this correlation is not the result of luck. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf075f1",
   "metadata": {},
   "source": [
    "## 2. Feature Correlation Check though Correlation Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e657d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the correlation matrix, excluding the label column that we know have Nan values\n",
    "columns = ['left-z-axis-(deg/s)-kurt', 'left-z-axis-(deg/s)-zcr', 'right-z-axis-(deg/s)-zcr', 'right-z-axis-(deg/s)-pkcnt', 'left-z-axis-(deg/s)-pkcnt']\n",
    "data = data.drop(columns=columns)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "axis_corr = sns.heatmap(data.corr(), vmin=-1, vmax=1, center=0, cmap='coolwarm', square=True, linewidths=0.1, annot_kws={'fontsize':10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac362eb6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3c0f6",
   "metadata": {},
   "source": [
    "## 3. Principle Component Analysis\n",
    "\n",
    "The result of PCA gives us a clear separability between the classes of Healthy and Stroke subjects, proving that the two classes can be classified effectively. \n",
    "| **Pros of PCA**                  | **Why It’s Useful**                          |\n",
    "| -------------------------------- | -------------------------------------------- |\n",
    "| Reveals structure in the data    | Shows healthy vs stroke are separable        |\n",
    "| Reduces dimensions               | Useful for visualization and fast models     |\n",
    "| Can improve performance          | Helps in small datasets if too many features |\n",
    "| Less overfitting risk            | Fewer dimensions = less noise/flexibility    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = data.drop(columns=['label'])\n",
    "y = data['label']\n",
    "\n",
    "# Step 2: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA (2 components for visualization)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 4: Plot the PCA projection\n",
    "pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['label'] = y\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='label', palette='Set2', s=100)\n",
    "plt.title('PCA Projection of Gait Features')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d21da",
   "metadata": {},
   "source": [
    "One of the main issue with using PCA is that the information is not useful in this contexts. So although PCA easily helps us classify the users, we can not interpret the data and provide insights that doctors can use to help the patients.\n",
    "\n",
    "| **Cons of PCA (in this use)**        | **Why It’s a Problem**                            |\n",
    "| -----------------------------------  | ------------------------------------------------- |\n",
    "| PC1/PC2 are **not interpretable**    | They are weighted sums of 50+ original features   |\n",
    "| No physical or clinical meaning      | We can’t say PC1 means stride time variability    |\n",
    "| Clinicians can’t act on it           | They want to know *which* aspect of gait is wrong |\n",
    "| Feature importance is hidden         | We lose clarity on what the model is using       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2f8ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208c3ccd",
   "metadata": {},
   "source": [
    "## 4. Random Forest and Recursive Feature Elimination  \n",
    "\n",
    "Recursive Feature Elimination fits a model like Random Forest that starts with all the input variables, then iteratively removes those with the weakest relationship with the output until the desired number of features is reached. It actually fits a model instead of just running statistical tests unlike the Univariate Testing. This is done in order to drop highly correlated features. Given that features X1 and X2 are highly correlated then they will have the same kind of effect on the output variable, where the change in performance observed tends to be negligible. \n",
    "\n",
    "The idea here is to reduce the computational complexity and the noise of data while again retaining the meaningful information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('final_dataset.csv')\n",
    "X = df.drop(columns=['subject_id', 'label'])\n",
    "y = df['label']\n",
    "\n",
    "# Standardize features and Split dataset\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train RandomForest and get the feature importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "importance_df = pd.DataFrame({'feature': X.columns, 'importance': rf.feature_importances_}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Recursive Feature Elimination and get feature importance\n",
    "rfe = RFE(estimator=rf, n_features_to_select=10)\n",
    "rfe.fit(X_train, y_train)\n",
    "rfe_df = pd.DataFrame({'feature': X.columns, 'selected_by_rfe': rfe.support_, 'ranking': rfe.ranking_}).sort_values(by='ranking')\n",
    "\n",
    "# Merge results\n",
    "merged = pd.merge(importance_df, rfe_df, on='feature', how='inner', validate='one_to_one')\n",
    "merged.sort_values(by='ranking')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd6d832",
   "metadata": {},
   "source": [
    "Based on the feature selection above we found that 11 out of the 58 features provide us the best subset of data to classify between the Healthy and Stroke subjects. The features that provide us the best chance are the following:\n",
    "\n",
    "| Feature                     | Source                        |\n",
    "|-----------------------------|-------------------------------|\n",
    "| right-z-axis-(deg/s)-min    | Point-biserial Correlation    |\n",
    "| left-z-axis-(deg/s)-iqr     | Recursive Feature Elimination |\n",
    "| left-z-axis-(deg/s)-min     | Recursive Feature Elimination |\n",
    "| left-z-axis-(deg/s)-rms     | Recursive Feature Elimination |\n",
    "| left_gait_band_energy_mean  | Recursive Feature Elimination |\n",
    "| left-z-axis-(deg/s)-std     | Recursive Feature Elimination |\n",
    "| right-z-axis-(deg/s)-max    | Recursive Feature Elimination |\n",
    "| left-z-axis-(deg/s)-max     | Recursive Feature Elimination |\n",
    "| right_gait_band_energy_mean | Recursive Feature Elimination |\n",
    "| right_gait_band_energy_std  | Recursive Feature Elimination |\n",
    "| right-z-axis-(deg/s)-range  | Recursive Feature Elimination |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b5d72",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b5c89",
   "metadata": {},
   "source": [
    "# Machine Learning Model\n",
    "\n",
    "Given the limited number of data we have to select Machine Learning models in order to get the best possible results. We are going to test a set of machine learning models like:\n",
    "1. Logistic Regression\n",
    "2. Random Forest\n",
    "3. Support Vector Machines\n",
    "\n",
    "Also we are going to use Leave-One-Out Cross-Validation (LOOCV) for maximum generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ab507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload dataset\n",
    "df = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "# Selected top features\n",
    "selected_features = [\n",
    "    'right-z-axis-(deg/s)-min',\n",
    "    'left-z-axis-(deg/s)-iqr',\n",
    "    'left-z-axis-(deg/s)-min',\n",
    "    'left-z-axis-(deg/s)-rms',\n",
    "    'left_gait_band_energy_mean',\n",
    "    'left-z-axis-(deg/s)-std',\n",
    "    'right-z-axis-(deg/s)-max',\n",
    "    'left-z-axis-(deg/s)-max',\n",
    "    'right_gait_band_energy_mean',\n",
    "    'right_gait_band_energy_std',\n",
    "    'right-z-axis-(deg/s)-range'\n",
    "]\n",
    "\n",
    "# Feature matrix and labels\n",
    "X = df[selected_features]\n",
    "y = df['label']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize LOOCV\n",
    "cv = LeaveOneOut()\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=100),\n",
    "    'Random Forest':       RandomForestClassifier(n_estimators=10, random_state=42),\n",
    "    'SVM (Linear Kernel)': SVC(kernel='linear')\n",
    "}\n",
    "\n",
    "# Evaluate models using LOOCV\n",
    "results = []\n",
    "for model_name, model in models.items():\n",
    "    scores = cross_val_score(model, X_scaled, y, cv=cv)\n",
    "    results.append({'model': model_name, 'mean_accuracy': scores.mean(), 'std_accuracy': scores.std(), 'fold_accuracies': scores.tolist()})\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('all_features_model_evaluation_results.csv', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19923367",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "The result of the training of the models shows that the use of all of these features results at a mean 100% accuracy and 0% standard deviation accuracy. This means that the models over-fit on the data, which would negatively affect the generalizability of the model with unseen or future data. Of course we need to take into consideration that we need to make a model that can be interpreted by medical professionals as well. Based on these criteria we have to choose a subset of features that is helpful to both the patients and the medical staff, with those being: \n",
    "1. Right and Left shank maximum swing speed deg/s\n",
    "2. Right and Left shank minimum swing speed deg/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload dataset\n",
    "df = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "# Selected top features\n",
    "selected_features = [\n",
    "    'right-z-axis-(deg/s)-min',\n",
    "    'left-z-axis-(deg/s)-min',\n",
    "    'right-z-axis-(deg/s)-max',\n",
    "    'left-z-axis-(deg/s)-max'\n",
    "]\n",
    "\n",
    "# Feature matrix and labels\n",
    "X = df[selected_features]\n",
    "y = df['label']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize LOOCV\n",
    "cv = LeaveOneOut()\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=100),\n",
    "    'Random Forest':       RandomForestClassifier(n_estimators=10, random_state=42),\n",
    "    \"SVM (Linear Kernel)\": SVC(kernel=\"linear\", probability=True)\n",
    "}\n",
    "\n",
    "# Evaluate models using LOOCV\n",
    "results = []\n",
    "for model_name, model in models.items():\n",
    "    scores = cross_val_score(model, X_scaled, y, cv=cv)\n",
    "    results.append({'model': model_name, 'mean_accuracy': scores.mean(), 'std_accuracy': scores.std(), 'fold_accuracies': scores.tolist()})\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('model_evaluation_results.csv', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529adb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select interpretable features for 2D plot\n",
    "X = df[['right-z-axis-(deg/s)-min', 'left-z-axis-(deg/s)-min']]\n",
    "y = df['label']\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create meshgrid for decision boundary\n",
    "x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "\n",
    "# Plot each model's decision boundary\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_scaled, y)\n",
    "    probs = model.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.contourf(xx, yy, probs, 25, cmap=\"coolwarm\", alpha=0.8)\n",
    "    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap=\"coolwarm\", edgecolors='k', s=100)\n",
    "    plt.xlabel(\"right-z-axis-(deg/s)-min (scaled)\")\n",
    "    plt.ylabel(\"left-z-axis-(deg/s)-min (scaled)\")\n",
    "    plt.title(f\"Decision Boundary ({model_name})\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select interpretable features for 2D plot\n",
    "X = df[['right-z-axis-(deg/s)-max', 'left-z-axis-(deg/s)-max']]\n",
    "y = df['label']\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create meshgrid for decision boundary\n",
    "x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "\n",
    "# Plot each model's decision boundary\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_scaled, y)\n",
    "    probs = model.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.contourf(xx, yy, probs, 25, cmap=\"coolwarm\", alpha=0.8)\n",
    "    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap=\"coolwarm\", edgecolors='k', s=100)\n",
    "    plt.xlabel(\"right-z-axis-(deg/s)-max (scaled)\")\n",
    "    plt.ylabel(\"left-z-axis-(deg/s)-max (scaled)\")\n",
    "    plt.title(f\"Decision Boundary ({model_name})\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
